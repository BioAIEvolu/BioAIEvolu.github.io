{"pages":[],"posts":[{"title":"【爬虫项目(二)】Selenium框架爬取淘宝商品信息","text":"特别声明： 供交流学习使用，不得用作商业用途。 如有违规侵权，请联系删除。一、Selenium框架介绍 Selenium 是一个用于浏览器自动化测试的框架，可以用来爬取任何网页上看到的数据。简单地说，就是模拟一个人所有的网页操作的行为 Selenium的下载与安装: 安装：在终端输入 pip install selenium 下载：下载Chromedriver，解压后放在…\\Google\\Chrome\\Application\\（右击Chrome图标，打开文件所在文件夹） 环境变量：将该目录添加至环境变量：右键点击我的电脑—–&gt;属性—&gt;高级系统设置—-&gt;环境变量——&gt;在path路径下添加上文中…\\Google\\Chrome\\Application\\路径 使用代码测试：Selenium的简单使用： 12345678from selenium import webdriverfrom selenium.webdriver.common.keys import Keysdriver = webdriver.Chrome()driver.get('http://www.baidu.com')elem = driver.find_element_by_xpath('//*[@id = &quot;kw&quot;]') #查找输入框elem.send_keys('Python Selenium',Keys.ENTER) #模拟点击回车print(driver.page_source) 遇到的问题：报错显示缺少参数”Python3 Selenium自动化测试赋值出现：WebDriverException: Message: unknown error: call function result missing ‘value’”，发现的Chromedriver版本要跟当前Chrome版本一致才可以 Chrome版本对应Chromedriver下载地址 Selenium的优缺点: 优点：Selenium可以爬取任何网页的任何内容，因为它是通过浏览器访问的方式进行数据的爬取，没有网站会拒绝浏览器的访问。 但是淘宝和知乎会对Selenium有反爬机制，需要进行伪装。 缺点：时间以及内存消耗太大,可以开启无头模式headless或者PhantomJS webdriver缓解这个问题 二、Selenium的使用跟人操作网页一样，都是需要查找和操作网页元素 查找元素XXX表示用CSS、id、name、identifier、XPath定位、 超链接、DOM、 CSS selector进行定位。更多定位方式PS: 对于网页的定位路径，只需要在谷歌浏览器中，鼠标放到对应元素--右键--检查--在右边的网页源码相应位置--右键--复制--选择相应的路径定位方法 1.直接用webdriver对象的查找1234driver.find_element_by_XXX() 查找符合条件的单个元素 driver.find_elements_by_XXX() 查找符合条件的一组元素 此方法通常需要前面编写time.sleep()一定秒数，让网页加载完毕，否则可能会找不到元素 WebDriverWait对象（更推荐）此方法可以显式等待网页加载显式等待可以自定义等待的条件，用于更加复杂的页面等待条件123456789from selenium import webdriverfrom selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Chrome()driver.get('填入网站的url')wait = WebDriverWait(driver, 10) 构建WebDriverWait对象，10秒加载时间wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'CSS选择器定位路径'))) # 很多有经验的selenium用户推荐CSS定位方式，因为它比XPath更快。而且可以在HTML文件中找到更复杂的对象。 使用pyquery库类似JQuery定位元素此方法适合用作返回网页元素进行下一步处理12345678910from selenium import webdriverfrom pyquery import PyQuery as pqdriver = webdriver.Chrome()driver.get('填入网站的url')html = driver.page_source()doc = pq(html)#pyquery(browser.page_source)就相当于requests.get获取的内容items = doc('CSS选择器路径').items() 操作元素相应的，我们有： 直接用webdriver对象的操作123456789from selenium import webdriveraccount = browser.find_element_by_xpath('//*[@id=&quot;TPL_username_1&quot;]')account.clear()account.send_keys(&quot;用户名&quot;)passwd = browser.find_element_by_xpath('//*[@id=&quot;TPL_password_1&quot;]')passwd.send_keys(&quot;密码&quot;)submit = browser.find_element_by_xpath('//*[@id=&quot;J_SubmitStatic&quot;]')submit.click() #点击登录按钮 WebDriverWait对象的操作（更推荐）也是可以显示等待加载时间12345678910from selenium import webdriverfrom selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as EC input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input')))submit = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit')))input.clear() # 清除输入框内容input.send_keys(page_num) # 输入输入框内容submit.click() # 点击确定按钮 三、Selenium项目实战（爬取淘宝商品）思路： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#! /usr/bin/env python# -*- coding:utf-8 -*-# Use for get infomation from taobao by ChromeDriver# Author:Robin; Created in 20190831from selenium import webdriverfrom selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException #检查网络请求是否超时#from selenium.webdriver import ChromeOptions#from selenium.webdriver.common.keys import Keys#from selenium.webdriver import ActionChainsfrom pyquery import PyQuery as pq from pymongo import MongoClient#import randomimport timeimport rebrowser = webdriver.Chrome() #创建webdriver对象wait = WebDriverWait(browser, 10) #进入淘宝网，输入商品名称，返回页面def search(good): try: browser.get('https://www.taobao.com/') input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#q'))) # 搜索输入框 submit = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#J_TSearchForm &gt; div.search-button &gt; button'))) # 搜索点击按 keys = '{}'.format(good) input.send_keys(keys) #输入商品名 submit.click() #点击搜索 total = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.total'))) get_products(good) return total.text #总页数 except TimeoutException: #login(browser) search(good) #跳转到下一页def next_page(page_num, good): print('正在爬取第{}页'.format(page_num)) try: input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input'))) #页码输入框 submit = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit'))) #页码跳转确定按钮 input.clear() #清空页码框 input.send_keys(page_num) #输入页码 submit.click() # 点击确定 time.sleep(10) #防止爬取过快 wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span'), str(page_num))) #检查是否跳转到正确页数 get_products(good) except TimeoutException: next_page(page_num, good)#得到淘宝商品信息def get_products(good): wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-itemlist .items .item'))) html = browser.page_source doc = pq(html) # pyquery(browser.page_source)就相当于requests.get获取的内容, 构建PyQuery对象 items = doc('#mainsrp-itemlist .items .item').items() #获取所有的‘#mainsrp-itemlist .items .item’选择出来的内容下的各项 for item in items: # 每个item变量都是一个PyQuery对象，然后再调用它的find()方法，传入CSS选择器，就可以获取单个商品的特定内容了。 products = { 'image': item.find('.pic .img').attr('src'), #获取图片链接 'price': item.find('.price').text(), 'deal': item.find('.deal-cnt').text()[:-3], 'title': item.find('.title').text(), 'shop': item.find('.shop').text(), 'location': item.find('.location').text(), } #print(products) save_to_mongo(products, good)# 保存数据到MongoDBdef save_to_mongo(result, good): client = MongoClient('mongodb://localhost:27017') db = client.taobao set_name = '{}'.format(good) goods_set = db[set_name] # 创建以商品名命名的数据集 try: if goods_set.insert(result): # 插入到MongoDB数据库相应数据集 print(&quot;存储到MONGODB成功&quot;,result) except Exception: print(&quot;存储到MONGODB失败&quot;,result)def main(good): print('正在爬取第1页...') total = search(good) total = int(re.compile('(\\d+)').search(total).group(1)) for i in range(2, total+1): next_page(i, good) browser.close() #把浏览器关掉if __name__ == '__main__': main('树莓派') 运行结果： 问题 淘宝模拟登录： 一开始我以为淘宝点击搜索商品是都会自动跳转到登录页面的，然后就想用Selenium模拟人的登录，然后再利用selenium.webdriver的ActionChains的click_and_hold(slider).perform() 和 click_and_hold(slider).perform() 方法，模拟人操作时候先快后慢的特点 后面才发现淘宝识别的是浏览器发送的一些参数判定是Selenium还是普通浏览器 但是感觉这个代码挺有意思的，或许以后也有用，也拿出来分享下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105from selenium import webdriverfrom selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException #检查网络请求是否超时from selenium.webdriver import ChromeOptionsfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver import ActionChainsfrom pyquery import PyQuery as pq from pymongo import MongoClientimport randomimport timeimport re##选用开发者模式，创建一个浏览器对象，可避免被淘宝检测到是selenium模拟浏览器option = ChromeOptions()option.add_argument('--proxy-serve=127.0.0.1:8080')option.add_experimental_option('excludeSwitches',['enable-automation']) #chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])#禁止打印日志 跟上面只能选一个browser = webdriver.Chrome(options=option)browser = webdriver.Chrome()wait = WebDriverWait(browser, 10)# 滑块移动轨迹计算：使得滑块先快后慢def get_track(distance): track = [] current = 0 mid = distance * (1/2) t = 2 v = 0 a = 4 a1 = -4 while current &lt; distance: #滑条前1/2的部分加速滑动 if current &lt; mid: move = v * t + 1/2 * a *t * t #物理上匀加速运动计算某一时间内的位移： s = v*t + (1/2)a*t^2 current += move track.append(round(move)) v = v + a*t # 加速运动物体某一时刻的瞬时速度 else: #滑条后1/2的部分加速度减慢 move = v * t + 1/2 * a1 *t *t current += move track.append(round(move)) v = v + a1*t return trackdef drag(length, xpath): if browser.find_element_by_xpath(&quot;{}&quot;.format(xpath)): slider = browser.find_element_by_xpath(&quot;{}&quot;.format(xpath)) #找到滑动按钮 track = get_track(length) #模拟运动轨迹，速度先快后慢 ActionChains(browser).click_and_hold(slider).perform() # 按住滑块滑动 for x in track: ActionChains(browser).drag_and_drop_by_offset(slider, xoffset=x, yoffset=random.randint(1,3)).perform() ActionChains(browser).release().perform() else: pass # Selenium控制的情况下搜索商品，淘宝会自动跳转到登录界面def login(browser): try: button = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#J_Quick2Static'))) button.click() account = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#TPL_username_1'))) passwd = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#TPL_password_1'))) submit = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#J_SubmitStatic'))) account.clear() account.send_keys('用户名') passwd.send_keys('密码') submit.click() try: change_login = browser.find_element_by_id('J_Quick2Static') #点击密码登录按钮，选择用密码方式登录 time.sleep(3) change_login.click() except Exception: pass time.sleep(3) account = browser.find_element_by_xpath('//*[@id=&quot;TPL_username_1&quot;]') account.clear() account.send_keys('用户名’) passwd = browser.find_element_by_xpath('//*[@id=&quot;TPL_password_1&quot;]') passwd.send_keys('密码') try: drag(700, &quot;//*[@id='nc_1_n1z']&quot;) except Exception: pass submit = browser.find_element_by_xpath('//*[@id=&quot;J_SubmitStatic&quot;]') submit.click() #点击登录按钮 try: index_page = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '//*[@id=&quot;J_SiteNavHome&quot;]/div/a/span'))) index_page.click() except Exception: login(browser) except TimeoutException: login(browser)if __name__ == '__main__': login(browser) 有时候用正则表达式匹配淘宝的页码，total = int(re.compile(‘(\\d+)’).search(total).group(1))会匹配失败，不知道究竟是加载不完全还是淘宝的一些反爬措施，有空再研究下 当爬取到一定页数的时候，淘宝会检查到页面的异常，然后就会卡在验证页面，出现手动也无法通过验证的情况，所以应该要多准备几个代理和IP，使用scrapy框架来爬取会更加方便。改进 由于淘宝对Selenium的反爬做得比较好，所以登录需要手动扫码登录，如果需要模拟登录请参考以下链接： python模拟登陆淘宝（更新版） 如果需要更新MongoDB数据库，请参考： MongoDB 更新文档 Mongodb数据更新命令（update、save） python 更新 MongoDB python与mongodb的交互 增删改差 账号和密码可以存储到json或者pickle打包的配置文件里面，需要时回调，方便代码维护和信息安全 更多Selenium技巧扩展阅读：可以用于掩饰Selenium，防止被识别出设置headless和关闭图片缓存，可以减少机器负荷，提高性能 - chrome配置设置无头浏览器PhantomJS和关闭图片缓存 - Windows下PhantomJS的安装和使用 - Python3网络爬虫开发实战 1.2.5-PhantomJS的安装 - Python爬虫利器四之PhantomJS的用法Selenium更多定位用法： - 菜鸟学自动化测试（五）—–selenium命令之定位页面元素更多CSS选择器用法：（PS：这又是一个大坑） - 【Selenium专题】元素定位之CssSelector - Selectors Level 3官方文档其他参考教程： - 网络爬虫（python项目） - 爬虫实践—Selenium-抓取淘宝搜索商品信息 - (九)使用Selenium+Chrome/PhantomJS(模拟浏览器)抓取淘宝商品美食信息|Python3网络爬虫开发实战","link":"/2020/02/06/%E3%80%90%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE2%E3%80%91Selenium%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/02/03/hello-world/"},{"title":"'个人博客搭建_1：Github+Hexo+Icarus主题选择'","text":"个人博客搭建(一)：Github+Hexo+Icarus主题优化 环境说明： Win10 已经安装和配置好git 已经有github账号了，本地windows的git环境已经配置好了 npm已经有了 检查： 123git versionnode -vnpm -v 第一步 环境准备 安装 NodeJS 一路next下去安装即可 安装Hexo1npm install -g hexo-cli 第二步 创建github repository repository的命名必须是: 自己的github用户名.github.io 然后在本地目录中新建一个目录,然后构建这个目录与github的repository的联系 第三步 用hexo框架搭建个人博客 进入在上面的git初始化后的目录 运行123hexo init myBlogcd myBlognpm install 新建完成后，指定文件夹的目录如下：123456789&gt; dir .\\.├── _config.yml # 网站的配置信息，您可以在此配置大部分的参数。 ├── package.json├── scaffolds # 模版文件夹├── source # 资源文件夹，除 _posts 文件，其他以下划线_开头的文件或者文件夹不会被编译打包到public文件夹| ├── _drafts # 草稿文件| └── _posts # 文章Markdowm文件 └── themes # 主题文件夹 如果没有报错，继续运行：123hexo s #或者 hexo server 在浏览器中输入 http://localhost:4000 回车就可以预览效果了。第四步 主题个性化 此处选择的主题是：icarus 进入themes文件夹12&gt; cd themes&gt; git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus 设置主题的个性化设置：（先跳过）第五步 将hexo的网页部署到 Github 修改配置，在 _config.yml文件下修改以下内容1234567# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: 'git' repo: github: https://github.com/BioAIEvolu/BioAIEvolu.github.io.git #新建仓库地址 branch: master #published 安装一个部署插件 hexo-deployer-git1npm install hexo-deployer-git --save 上传到github，以下 g 是 generate 缩写，d 是 deploy 缩写：1hexo g -d 在浏览器访问网址： https://你的用户名.github.io 就会看到你的博客 第六步 用VScode编写Markdown文章并上传 打开VScode,安装插件”Markdown Preview enhanced” 新建文章：在cmd输入命令12345&gt; hexo new '创建个人博客'=============================================INFO Checking dependenciesINFO Validating the configuration fileINFO Created: D:\\Data\\blog\\myblog\\source\\_posts\\创建个人博客.md 用VScode打开文件D:\\Data\\blog\\myblog\\source\\_posts\\创建个人博客.md 本地检查12hexo ghexo s 部署到Github 12hexo clean #清除缓存文件 (db.json) 和已生成的静态文件 (public)hexo g -d 参考文章 https://segmentfault.com/a/1190000017986794 https://blog.csdn.net/marvine/article/details/89816846 https://blog.csdn.net/Fzidx/article/details/99185663 https://github.com/ppoffice/hexo-theme-icarus https://www.cnblogs.com/KongkOngL/p/10449269.html","link":"/2020/02/06/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA-1%EF%BC%9AGithub-Hexo-Icarus%E4%B8%BB%E9%A2%98%E9%80%89%E6%8B%A9/"}],"tags":[],"categories":[]}